def build_baseline():
  model=keras.Sequential(
    [
        # First Convolution Layer & Max Pooling Layer  
        layers.Conv2D(32, (3, 3), activation='relu', padding="same",input_shape=(150, 150, 3)),
        layers.BatchNormalization(),
        layers.MaxPooling2D((2, 2)),
        # Second Convolution Layer & Max Pooling Layer
        layers.Conv2D(64, (3, 3), padding="same", activation='relu'),
        layers.BatchNormalization(),
        layers.MaxPooling2D((2, 2)),

        # Third Convolution Layer & Max Pooling Layer
        layers.Conv2D(128, (3, 3), padding="same",activation='relu'),
        layers.BatchNormalization(),
        layers.MaxPooling2D((2, 2)),
        # Fourth Convolution Layer & Max Pooling Layer
        layers.Conv2D(128, (3, 3), padding="same",activation='relu'),
        layers.BatchNormalization(),
        layers.MaxPooling2D((2, 2)),

        #Global Average Pooling Layer
        layers.GlobalAveragePooling2D(),

        # Sigmoid Activation in a Dense Output Layer   
        layers.Dense(6, activation= 'softmax')
    ])
  
# Model Compilation
  
  #Learning Rate Scheduling
  lr_schedule = keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate=1e-4,
    decay_steps=1000,
    decay_rate=0.9)

  #Configuration and compilation
  opt = tf.keras.optimizers.Adam(learning_rate=lr_schedule)
  model.compile(loss='sparse_categorical_crossentropy', metrics=['accuracy'], optimizer=opt)
 
  return model

model=build_baseline()
print(model.summary())
