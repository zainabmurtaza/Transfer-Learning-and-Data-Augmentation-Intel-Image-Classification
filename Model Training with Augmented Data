"""
It is apparent that there is no prevalence of overfitting in the model. 
Validation accuracy, despite it's sharp upward and downward peaks is projected to be better than the Training accuracy. 
The accuracy in our previous plots meandered at around a value of 85%, but we see here that it has come down to around 82% or so. 
"""

model=keras.Sequential(
  [
      # First Convolutional Layer & Max Pooling Layer 
      layers.Conv2D(32, (3, 3), activation='relu', padding="same",input_shape=(150, 150, 3)),
      layers.BatchNormalization(),
      layers.MaxPooling2D((2, 2)),
      # Second Convolutional Layer & Max Pooling Layer
      layers.Conv2D(64, (3, 3), padding="same", activation='relu'),
      layers.BatchNormalization(),
      layers.MaxPooling2D((2, 2)),
      # Third Convolutional Layer & Max Pooling Layer
      layers.Conv2D(128, (3, 3), padding="same",activation='relu'),
      layers.BatchNormalization(),
      layers.MaxPooling2D((2, 2)),
      # Fourth Convolutional Layer & Max Pooling Layer
      layers.Conv2D(256, (3, 3), padding="same",activation='relu'),
      layers.BatchNormalization(),
      layers.MaxPooling2D((2, 2)),
      # Fifth Convolutional Layer & Max Pooling Layer
      layers.Conv2D(256, (3, 3), padding="same",activation='relu'),
      layers.BatchNormalization(),
      layers.MaxPooling2D((2, 2)),
      #Global Average Pooling Layer
      layers.GlobalAveragePooling2D(),
      # First Fully Connected Layer/Dense Layer
      layers.Dense(128, activation='relu'),
      # Initial Dropout
      layers.Dropout(0.5),
      # Second Fully Connected Layer/Dense Layer
      layers.Dense(64, activation='relu'),
      # Second Dropout
      layers.Dropout(0.5),
      # Sigmoid Activation in Dense Output Layer  
      layers.Dense(6, activation= 'softmax')
  ])

#Compilation

#Learning Rate Schedule
lr_schedule = keras.optimizers.schedules.ExponentialDecay(
  initial_learning_rate=1e-4,
  decay_steps=1000,
  decay_rate=0.9)

#Configuration
opt = tf.keras.optimizers.Adam(learning_rate=lr_schedule)
model.compile(loss='sparse_categorical_crossentropy', metrics=['accuracy'], optimizer=opt)

#Early Stopping Callback
callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, min_delta=1e-3, restore_best_weights=True)

#Training
history = model.fit(
            train_generator,
            validation_data = validation_generator,
            steps_per_epoch = math.ceil(train_generator.samples/BATCH_SIZE) ,
            epochs = 120,
            validation_steps = math.ceil(validation_generator.samples/BATCH_SIZE),
            callbacks=callback,
            verbose = 1)
