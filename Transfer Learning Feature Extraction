"""
These graphs illustrate that the pre-trained model pertains high efficiency in comparison to our previous examples. 
The model does not overfit. 
Validation Accuracy can be recorded at 90% and fairly steady. There are no harsh varying peaks.
"""

#Importing the Trained Model

#Using a DenseNet121 model trained on ImageNet dataset
from tensorflow.keras.applications import DenseNet121
conv_base = DenseNet121(weights='imagenet', include_top=False, input_shape=(150, 150, 3))

#Freezing convolutional base weight
conv_base.trainable=False

#Adding a Fully Connected/Dense Layer at the beginning
#Over the convolutional base (densenet) outpupt, set a topclassifier.
topClassifier = conv_base.output

#On top of the convolutional base, set a Global Average Pooling layer.
topClassifier=layers.GlobalAveragePooling2D()(topClassifier)

#For classification purposes, place and incorporate a dense layer preceding the Global Average Pooling layer.
topClassifier = layers.Dense(6, activation="softmax")(topClassifier)

model = Model(inputs=conv_base.input, outputs=topClassifier)

#Training

#Learning Rate Scheduling
lr_schedule = keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate=1e-4,
    decay_steps=1000,
    decay_rate=0.9)

#Compilation of Model
opt = tf.keras.optimizers.Adam(learning_rate=lr_schedule)
model.compile(loss="sparse_categorical_crossentropy", metrics=['accuracy'], optimizer=opt)

#Early Stopping Callback
callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, min_delta=1e-3, restore_best_weights=True)

#Training
history = model.fit(
            train_generator,
            validation_data = validation_generator,
            steps_per_epoch = math.ceil(train_generator.samples/BATCH_SIZE) ,
            epochs = 120,
            validation_steps = math.ceil(validation_generator.samples/BATCH_SIZE),
            callbacks=callback,
            verbose = 1)
