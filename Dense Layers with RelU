"""
The addition of dense layers preceding the output layer aided regularization within the network and reversed overfitting. 
In comparison to our previous learning curve plots, we see here that the validation accuracy is far lower than training accuracy. Colab notebook for visualization:
https://colab.research.google.com/drive/15hYxHjqx4hh8ydcdzL0ryZ_C22G20Dlt#scrollTo=SbTrb3JKjcAa
"""

model=keras.Sequential(
  [
      # First Convolutional Layer & Max Pooling Layer  
      layers.Conv2D(32, (3, 3), activation='relu', padding="same",input_shape=(150, 150, 3)),
      layers.BatchNormalization(),
      layers.MaxPooling2D((2, 2)),
      # Second Convolutional Layer & Max Pooling Layer
      layers.Conv2D(64, (3, 3), padding="same", activation='relu'),
      layers.BatchNormalization(),
      layers.MaxPooling2D((2, 2)),
      # Third Convolutional Layer & Max Pooling Layer
      layers.Conv2D(128, (3, 3), padding="same",activation='relu'),
      layers.BatchNormalization(),
      layers.MaxPooling2D((2, 2)),
      # Fourth Convolutional Layer & Max Pooling Layer
      layers.Conv2D(256, (3, 3), padding="same",activation='relu'),
      layers.BatchNormalization(),
      layers.MaxPooling2D((2, 2)),
      # Fifth Convolutional Layer & Max Pooling Layer
      layers.Conv2D(256, (3, 3), padding="same",activation='relu'),
      layers.BatchNormalization(),
      layers.MaxPooling2D((2, 2)),
      #Global Average Pooling Layer
      layers.GlobalAveragePooling2D(),
      # Fully Connected Layer / Dense Layer
      layers.Dense(128, activation='relu'),
      # Initial Dropout
      layers.Dropout(0.5),
      # Second Fully Connected Layer / Dense Layer 2
      layers.Dense(64, activation='relu'),
      # Second Dropout
      layers.Dropout(0.5),
      # Sigmoid Activation in Dense Output Layer   
      layers.Dense(6, activation= 'softmax')
  ])

#Model Compilation

#Learning Rate Schedule
lr_schedule = keras.optimizers.schedules.ExponentialDecay(
  initial_learning_rate=1e-4,
  decay_steps=1000,
  decay_rate=0.9)

#Compilation
opt = tf.keras.optimizers.Adam(learning_rate=lr_schedule)
model.compile(loss='sparse_categorical_crossentropy', metrics=['accuracy'], optimizer=opt)

#Early Stopping Callback
callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, min_delta=1e-3, restore_best_weights=True)

#Training Model
history = model.fit(
            train_generator,
            validation_data = validation_generator,
            steps_per_epoch = math.ceil(train_generator.samples/BATCH_SIZE) ,
            epochs = 120,
            validation_steps = math.ceil(validation_generator.samples/BATCH_SIZE),
            callbacks=callback,
            verbose = 1)
