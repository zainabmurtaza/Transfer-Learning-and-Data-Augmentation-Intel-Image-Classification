"""
These two NEW plotted graphs illustrate some overfitting, now that we have increased model capacity. 
There is also a slight difference in the prevalence of variating increase-decrease peaks in validation accuracy/losses, 
thereby concluding that adding a few convolutional layers was benefitial to the model overall. Training accuracies and losses remain marginally the same.
To view graphs please visit Colaboratory link.
"""

model=keras.Sequential(
  [
      # First Convolutional Layer & Max Pooling Layer  
      layers.Conv2D(32, (3, 3), activation='relu', padding="same",input_shape=(150, 150, 3)),
      layers.BatchNormalization(),
      layers.MaxPooling2D((2, 2)),
      # Second Convolutional Layer & Max Pooling Layer 
      layers.Conv2D(64, (3, 3), padding="same", activation='relu'),
      layers.BatchNormalization(),
      layers.MaxPooling2D((2, 2)),
      # Third Convolutional Layer & Max Pooling Layer 
      layers.Conv2D(128, (3, 3), padding="same",activation='relu'),
      layers.BatchNormalization(),
      layers.MaxPooling2D((2, 2)),
      # Fourth Convolutional Layer & Max Pooling Layer 
      layers.Conv2D(256, (3, 3), padding="same",activation='relu'),
      layers.BatchNormalization(),
      layers.MaxPooling2D((2, 2)),
      # Fifth Convolutional Layer & Max Pooling Layer 
      layers.Conv2D(256, (3, 3), padding="same",activation='relu'),
      layers.BatchNormalization(),
      layers.MaxPooling2D((2, 2)),
      #Global Average Pooling Layer
      layers.GlobalAveragePooling2D(),

      #Sigmoid Activation in a Dense Output Layer   
      layers.Dense(6, activation= 'softmax')
  ])

# Model Compilation

#Learning Rate Scheduling
lr_schedule = keras.optimizers.schedules.ExponentialDecay(
  initial_learning_rate=1e-4,
  decay_steps=1000,
  decay_rate=0.9)

#Model Compilation & Configuration 
opt = tf.keras.optimizers.Adam(learning_rate=lr_schedule)
model.compile(loss='sparse_categorical_crossentropy', metrics=['accuracy'], optimizer=opt)

#Early Stopping Callback
callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, min_delta=1e-3, restore_best_weights=True)

#Training
history = model.fit(
            train_generator,
            validation_data = validation_generator,
            steps_per_epoch = math.ceil(train_generator.samples/BATCH_SIZE) ,
            epochs = 100,
            validation_steps = math.ceil(validation_generator.samples/BATCH_SIZE),
            callbacks=callback,
            verbose = 1)
